# 架构
将transformer中的FNN层替换为MoE层，多个FNN,通过门控单元实现选择
稀疏激活。
多个专家适合多任务学习
缺点是多个专家需要的显存也很多

# 问题
## 稀疏激活，怎么选择激活哪些专家
router来决定，FNN+softmax，输出一个选择概率，也可以输出多个。
训练的时候为了避免总是选择某几个专家，可以加入高斯噪声或者添加辅助loss来保证专家的平衡
推理的时候topk实现，采用给定负无穷，使得softmax为0
## 结构问题，是不是把FNN中的神经元分组实现不同专家
是的，把 FNN 的神经元分组，可以理解为实现“不同专家”，这正是 Mixture of Experts 的思路。